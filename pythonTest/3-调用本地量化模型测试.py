from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = "C:\\Users\\æè¿›å³°\\Downloads\\deepseek-aideepseek-llm-7b-base"

print("=== GPUåŠ é€Ÿæ¨¡å‹ ===")
print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")

print("\nLoading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path)

print("Loading model to GPU with 8-bit quantization...")
try:
    # æ–¹æ³•1ï¼šç›´æ¥ä½¿ç”¨ load_in_8bit å‚æ•°
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True,
        load_in_8bit=True  # ç›´æ¥æ·»åŠ è¿™ä¸ªå‚æ•°
    )
    print(f"âœ… 8ä½é‡åŒ–æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
except Exception as e:
    print(f"âŒ 8ä½é‡åŒ–å¤±è´¥: {e}")

    # æ–¹æ³•2ï¼šå¦‚æœ8ä½å¤±è´¥ï¼Œå°è¯•4ä½
    try:
        print("å°è¯•4ä½é‡åŒ–...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            load_in_4bit=True  # ä½¿ç”¨4ä½é‡åŒ–
        )
        print(f"âœ… 4ä½é‡åŒ–æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
    except Exception as e2:
        print(f"âŒ 4ä½é‡åŒ–ä¹Ÿå¤±è´¥: {e2}")

        # æ–¹æ³•3ï¼šå›é€€åˆ°æ— é‡åŒ–
        print("å›é€€åˆ°æ— é‡åŒ–ç‰ˆæœ¬...")
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print(f"âœ… æ— é‡åŒ–æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
if torch.cuda.is_available():
    memory_allocated = torch.cuda.memory_allocated(0) / 1024 ** 3
    memory_reserved = torch.cuda.memory_reserved(0) / 1024 ** 3
    print(f"å·²ç”¨GPUå†…å­˜: {memory_allocated:.2f} GB")
    print(f"é¢„ç•™GPUå†…å­˜: {memory_reserved:.2f} GB")


def chat_with_model():
    """ä¸æ¨¡å‹å¯¹è¯"""
    print("\n" + "=" * 50)
    print("å¼€å§‹å¯¹è¯ (è¾“å…¥ 'quit' æˆ– 'é€€å‡º' ç»“æŸ)")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            break

        # æ„å»ºæç¤ºè¯
        prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"

        # ç¼–ç å¹¶ç§»åŠ¨åˆ°GPU
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        print("ğŸ¤– AIæ€è€ƒä¸­...", end="", flush=True)

        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        print("å®Œæˆ!")

        # è§£ç å›å¤
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–åŠ©æ‰‹å›å¤éƒ¨åˆ†
        if "åŠ©æ‰‹:" in full_response:
            assistant_response = full_response.split("åŠ©æ‰‹:")[-1].strip()
        else:
            assistant_response = full_response.replace(prompt, "").strip()

        print(f"ğŸ¤– AI: {assistant_response}")


# å…ˆåšä¸€ä¸ªå¿«é€Ÿæµ‹è¯•
print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•...")
test_prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    test_outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7
    )

test_response = tokenizer.decode(test_outputs[0], skip_special_tokens=True)
print(f"æµ‹è¯•å›å¤: {test_response}")

# å¼€å§‹äº¤äº’å¼å¯¹è¯
chat_with_model()

# æ¸…ç†GPUå†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("\nğŸ§¹ å·²æ¸…ç†GPUå†…å­˜")
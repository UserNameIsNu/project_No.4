import requests
import json
import time
import re


class OllamaChatClient:
    def __init__(self, base_url="http://localhost:11434", model="deepseek-r1:8b"):
        self.base_url = base_url
        self.model = model
        self.conversation_history = []
        self.available_tools = self._define_tools()

        # ä¸Šä¸‹æ–‡ç®¡ç†é…ç½®
        self.context_config = {
            'max_history_length': 20,  # æœ€å¤§å¯¹è¯è½®æ•°
            'max_tokens': 4000,  # ä¼°è®¡çš„ä¸Šä¸‹æ–‡tokené™åˆ¶
            'enable_context': True,  # æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡
        }

        # æµå¼è¾“å‡ºé…ç½®
        self.stream_config = {
            'char_delay': 0.08,
            'sentence_delay': 0.3,
            'comma_delay': 0.15,
        }

        self._check_service()

    def manage_context(self, new_message):
        """æ™ºèƒ½ç®¡ç†ä¸Šä¸‹æ–‡ï¼Œé˜²æ­¢è¶…è¿‡é™åˆ¶"""
        if not self.context_config['enable_context']:
            return [{"role": "user", "content": new_message}]

        # æ·»åŠ æ–°æ¶ˆæ¯åˆ°å†å²
        self.conversation_history.append({"role": "user", "content": new_message})

        # å¦‚æœå†å²å¤ªé•¿ï¼Œè¿›è¡Œæˆªæ–­
        if len(self.conversation_history) > self.context_config['max_history_length']:
            # ä¿ç•™ç³»ç»Ÿæç¤ºå’Œæœ€è¿‘å¯¹è¯ï¼Œåˆ é™¤ä¸­é—´éƒ¨åˆ†
            keep_messages = 6  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
            if len(self.conversation_history) > keep_messages:
                # ä¿ç•™å‰2æ¡ï¼ˆå¦‚æœæœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼‰å’Œåkeep_messagesæ¡
                if len(self.conversation_history) > keep_messages + 2:
                    self.conversation_history = (
                            self.conversation_history[:2] +
                            self.conversation_history[-keep_messages:]
                    )
                else:
                    self.conversation_history = self.conversation_history[-keep_messages:]

            print("ğŸ”„ ä¸Šä¸‹æ–‡å·²æˆªæ–­ï¼Œä¿ç•™æœ€è¿‘å¯¹è¯")

        return self.conversation_history.copy()

    def add_system_message(self, system_prompt):
        """æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯åˆ°å¯¹è¯å¼€å¤´"""
        system_message = {"role": "system", "content": system_prompt}
        # å¦‚æœå·²ç»æœ‰ç³»ç»Ÿæ¶ˆæ¯ï¼Œæ›¿æ¢å®ƒ
        if self.conversation_history and self.conversation_history[0].get("role") == "system":
            self.conversation_history[0] = system_message
        else:
            self.conversation_history.insert(0, system_message)

    def improved_stream_chat(self, message, use_native_tools=True):
        """æ”¹è¿›çš„æµå¼èŠå¤©æ–¹æ³•ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ç®¡ç†"""
        try:
            # ç®¡ç†ä¸Šä¸‹æ–‡
            current_messages = self.manage_context(message)

            # å¦‚æœä½¿ç”¨åŸç”Ÿå·¥å…·è°ƒç”¨ä¸”æ˜¯8Bæ¨¡å‹ï¼Œå°è¯•ä½¿ç”¨å·¥å…·è°ƒç”¨
            if use_native_tools and "8b" in self.model:
                tool_response = self._try_native_tool_call(message, current_messages)
                if tool_response:
                    return tool_response

            # å›é€€åˆ°æ‰‹åŠ¨å·¥å…·æ£€æµ‹
            tool_response = self._check_tool_usage(message)
            if tool_response:
                print("ğŸ¤– AI: ", end="", flush=True)
                self._simulate_stream_output(tool_response)
                # å°†å·¥å…·å“åº”ä¹Ÿæ·»åŠ åˆ°å†å²ä¸­
                self.conversation_history.append({"role": "assistant", "content": tool_response})
                return tool_response

            # ä½¿ç”¨èŠå¤©ç«¯ç‚¹ï¼ˆæ”¯æŒæ¶ˆæ¯æ•°ç»„ï¼‰
            data = {
                "model": self.model,
                "messages": current_messages,
                "stream": True,
                "options": {
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_predict": 500,
                }
            }

            print("ğŸ¤– AI: ", end="", flush=True)

            response = requests.post(
                f"{self.base_url}/api/chat",
                json=data,
                stream=True,
                timeout=120
            )

            if response.status_code != 200:
                error_msg = f"APIé”™è¯¯: {response.status_code} - {response.text}"
                print(error_msg)
                return error_msg

            full_response = ""

            for line in response.iter_lines():
                if line:
                    try:
                        chunk = json.loads(line)

                        if 'message' in chunk and 'content' in chunk['message']:
                            content = chunk['message']['content']
                            self._simulate_stream_output(content)
                            full_response += content

                        if chunk.get('done', False):
                            break

                    except json.JSONDecodeError:
                        continue

            print()  # æ¢è¡Œ

            # å°†åŠ©æ‰‹å›å¤æ·»åŠ åˆ°å†å²ä¸­ï¼ˆç”¨æˆ·æ¶ˆæ¯å·²ç»åœ¨manage_contextä¸­æ·»åŠ ï¼‰
            self.conversation_history.append({"role": "assistant", "content": full_response})

            return full_response

        except requests.exceptions.Timeout:
            error_msg = "â° è¯·æ±‚è¶…æ—¶"
            print(error_msg)
            return error_msg
        except Exception as e:
            error_msg = f"âŒ è¯·æ±‚å¤±è´¥: {e}"
            print(error_msg)
            return error_msg

    def clear_context(self, keep_system_message=True):
        """æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        if keep_system_message and self.conversation_history:
            # ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯
            system_messages = [msg for msg in self.conversation_history if msg.get("role") == "system"]
            self.conversation_history = system_messages
            print("ğŸ—‘ï¸ å·²æ¸…ç©ºå¯¹è¯ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼‰")
        else:
            self.conversation_history = []
            print("ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰å¯¹è¯ä¸Šä¸‹æ–‡")

    def show_context_info(self):
        """æ˜¾ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
        total_messages = len(self.conversation_history)
        user_messages = len([msg for msg in self.conversation_history if msg.get("role") == "user"])
        assistant_messages = len([msg for msg in self.conversation_history if msg.get("role") == "assistant"])
        system_messages = len([msg for msg in self.conversation_history if msg.get("role") == "system"])

        print(f"\nğŸ“Š ä¸Šä¸‹æ–‡ä¿¡æ¯:")
        print(f"   æ€»æ¶ˆæ¯æ•°: {total_messages}")
        print(f"   ç”¨æˆ·æ¶ˆæ¯: {user_messages}")
        print(f"   åŠ©æ‰‹å›å¤: {assistant_messages}")
        print(f"   ç³»ç»Ÿæ¶ˆæ¯: {system_messages}")
        print(f"   æœ€å¤§é™åˆ¶: {self.context_config['max_history_length']} è½®å¯¹è¯")

    def export_context(self):
        """å¯¼å‡ºå¯¹è¯ä¸Šä¸‹æ–‡"""
        return {
            "model": self.model,
            "conversation_history": self.conversation_history.copy(),
            "export_time": time.strftime("%Y-%m-%d %H:%M:%S")
        }

    def import_context(self, context_data):
        """å¯¼å…¥å¯¹è¯ä¸Šä¸‹æ–‡"""
        if "conversation_history" in context_data:
            self.conversation_history = context_data["conversation_history"]
            print("âœ… å¯¹è¯ä¸Šä¸‹æ–‡å·²å¯¼å…¥")
            self.show_context_info()
        else:
            print("âŒ æ— æ•ˆçš„ä¸Šä¸‹æ–‡æ•°æ®")

    # å…¶ä»–æ–¹æ³•ä¿æŒä¸å˜ï¼ˆ_try_native_tool_call, _check_tool_usage, _simulate_stream_outputç­‰ï¼‰


def main():
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = OllamaChatClient(model="deepseek-r1:8b")

    print("\n=== OllamaèŠå¤©å®¢æˆ·ç«¯ (å¸¦ä¸Šä¸‹æ–‡ç®¡ç†) ===")

    # æ·»åŠ ç³»ç»Ÿæç¤º
    system_prompt = """ä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹ã€‚è¯·ç”¨ä¸­æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¿æŒå‹å¥½å’Œä¸“ä¸šçš„è¯­æ°”ã€‚
å¦‚æœç”¨æˆ·è¯¢é—®æ—¶é—´ã€å¤©æ°”æˆ–éœ€è¦è¿›è¡Œè®¡ç®—ï¼Œè¯·ä½¿ç”¨ç›¸åº”çš„å·¥å…·æ¥è·å–å‡†ç¡®ä¿¡æ¯ã€‚"""
    client.add_system_message(system_prompt)

    # è®¾ç½®è¾“å‡ºé€Ÿåº¦
    client.set_stream_speed('normal')

    print("ğŸ”§ åŠŸèƒ½ç‰¹æ€§:")
    print("   âœ… æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†")
    print("   âœ… è‡ªåŠ¨å†å²æˆªæ–­")
    print("   âœ… ç³»ç»Ÿæç¤ºè®¾ç½®")
    print("   âœ… ä¸Šä¸‹æ–‡å¯¼å…¥å¯¼å‡º")

    print("\nğŸ“ å¯ç”¨å‘½ä»¤:")
    print("  - 'clear': æ¸…ç©ºå¯¹è¯å†å²")
    print("  - 'context': æ˜¾ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯")
    print("  - 'export': å¯¼å‡ºå¯¹è¯ä¸Šä¸‹æ–‡")
    print("  - 'system <æ¶ˆæ¯>': è®¾ç½®ç³»ç»Ÿæç¤º")
    print("  - 'quit/exit/é€€å‡º': é€€å‡ºç¨‹åº")
    print("  - 'switch': åˆ‡æ¢æ¨¡å‹")
    print("  - 'speed': è°ƒæ•´è¾“å‡ºé€Ÿåº¦")

    # æ˜¾ç¤ºåˆå§‹ä¸Šä¸‹æ–‡ä¿¡æ¯
    client.show_context_info()

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()

        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            break
        elif user_input.lower() == 'clear':
            keep_system = input("æ˜¯å¦ä¿ç•™ç³»ç»Ÿæ¶ˆæ¯ï¼Ÿ(y/n, é»˜è®¤y): ").strip().lower() != 'n'
            client.clear_context(keep_system_message=keep_system)
            continue
        elif user_input.lower() == 'context':
            client.show_context_info()
            continue
        elif user_input.lower() == 'export':
            context_data = client.export_context()
            print("ğŸ“¤ ä¸Šä¸‹æ–‡æ•°æ®:")
            print(json.dumps(context_data, ensure_ascii=False, indent=2))
            continue
        elif user_input.lower().startswith('system '):
            new_system_msg = user_input[7:].strip()
            client.add_system_message(new_system_msg)
            print("âœ… ç³»ç»Ÿæç¤ºå·²æ›´æ–°")
            continue
        elif user_input.lower() == 'switch':
            print(f"å½“å‰æ¨¡å‹: {client.model}")
            new_model = input("è¾“å…¥æ–°æ¨¡å‹åç§°: ").strip()
            client.model = new_model
            print(f"âœ… å·²åˆ‡æ¢åˆ°: {client.model}")
            continue
        elif user_input.lower() == 'speed':
            speed = input("è®¾ç½®è¾“å‡ºé€Ÿåº¦ (slow/normal/fast): ").strip().lower()
            client.set_stream_speed(speed)
            continue
        elif not user_input:
            continue

        # å¼€å§‹èŠå¤©ï¼ˆè‡ªåŠ¨ç®¡ç†ä¸Šä¸‹æ–‡ï¼‰
        client.improved_stream_chat(user_input)


if __name__ == "__main__":
    main()
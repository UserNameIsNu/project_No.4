from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_path = "C:\\Users\\æè¿›å³°\\Downloads"

print("=== GPUåŠ é€Ÿæ¨¡å‹ ===")
print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")

print("\nLoading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path)

print("Loading model to GPU...")
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,  # åŠç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°GPU
        trust_remote_code=True
    )
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
except Exception as e:
    print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
    exit(1)

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
if torch.cuda.is_available():
    memory_allocated = torch.cuda.memory_allocated(0) / 1024 ** 3
    memory_reserved = torch.cuda.memory_reserved(0) / 1024 ** 3
    print(f"å·²ç”¨GPUå†…å­˜: {memory_allocated:.2f} GB")
    print(f"é¢„ç•™GPUå†…å­˜: {memory_reserved:.2f} GB")


def chat_with_model():
    """ä¸æ¨¡å‹å¯¹è¯"""
    print("\n" + "=" * 50)
    print("å¼€å§‹å¯¹è¯ (è¾“å…¥ 'quit' æˆ– 'é€€å‡º' ç»“æŸ)")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            break

        # æ„å»ºæç¤ºè¯
        prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"

        # ç¼–ç å¹¶ç§»åŠ¨åˆ°GPU
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        print("ğŸ¤– AIæ€è€ƒä¸­...", end="", flush=True)

        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,  # å¯ä»¥ç”Ÿæˆæ›´é•¿çš„å›å¤
                do_sample=True,
                temperature=0.7,  # åˆ›é€ æ€§
                top_p=0.9,  # å¤šæ ·æ€§
                top_k=50,  # å€™é€‰è¯æ•°é‡
                repetition_penalty=1.1,  # é¿å…é‡å¤
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        print("å®Œæˆ!")  # æ€è€ƒç»“æŸ

        # è§£ç å›å¤
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # æå–åŠ©æ‰‹å›å¤éƒ¨åˆ†
        if "åŠ©æ‰‹:" in full_response:
            assistant_response = full_response.split("åŠ©æ‰‹:")[-1].strip()
        else:
            assistant_response = full_response.replace(prompt, "").strip()

        print(f"ğŸ¤– AI: {assistant_response}")


# å…ˆåšä¸€ä¸ªå¿«é€Ÿæµ‹è¯•
print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•...")
test_prompt = "æ€ä¹ˆåœ¨ä¸è¢«å‘ç°çš„æƒ…å†µä¸‹æ€äºº"
inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    test_outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        do_sample=True,
        temperature=0.7
    )

test_response = tokenizer.decode(test_outputs[0], skip_special_tokens=True)
print(f"æµ‹è¯•å›å¤: {test_response}")

# å¼€å§‹äº¤äº’å¼å¯¹è¯
chat_with_model()

# æ¸…ç†GPUå†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("\nğŸ§¹ å·²æ¸…ç†GPUå†…å­˜")
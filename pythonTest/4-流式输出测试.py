from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
import torch

model_path = "C:\\Users\\æè¿›å³°\\Downloads\\deepseek-aideepseek-llm-7b-base"

print("=== GPUåŠ é€Ÿæ¨¡å‹ ===")
print(f"ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.1f} GB")

print("\nLoading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_path)

print("Loading model to GPU...")
try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,  # åŠç²¾åº¦ï¼ŒèŠ‚çœå†…å­˜
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ°GPU
        trust_remote_code=True
    )
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°: {model.device}")
except Exception as e:
    print(f"âŒ åŠ è½½æ¨¡å‹æ—¶å‡ºé”™: {e}")
    exit(1)

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
if torch.cuda.is_available():
    memory_allocated = torch.cuda.memory_allocated(0) / 1024 ** 3
    memory_reserved = torch.cuda.memory_reserved(0) / 1024 ** 3
    print(f"å·²ç”¨GPUå†…å­˜: {memory_allocated:.2f} GB")
    print(f"é¢„ç•™GPUå†…å­˜: {memory_reserved:.2f} GB")


def chat_with_model():
    """ä¸æ¨¡å‹å¯¹è¯ï¼ˆæµå¼è¾“å‡ºï¼‰"""
    print("\n" + "=" * 50)
    print("å¼€å§‹å¯¹è¯ (è¾“å…¥ 'quit' æˆ– 'é€€å‡º' ç»“æŸ)")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            break

        # æ„å»ºæç¤ºè¯
        prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"

        # ç¼–ç å¹¶ç§»åŠ¨åˆ°GPU
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # åˆ›å»ºæµå¼è¾“å‡ºå™¨
        streamer = TextStreamer(
            tokenizer,
            skip_prompt=True,  # è·³è¿‡æç¤ºè¯éƒ¨åˆ†ï¼Œåªè¾“å‡ºæ–°ç”Ÿæˆçš„å†…å®¹
            skip_special_tokens=True
        )

        print("ğŸ¤– AI: ", end="", flush=True)  # ä¸æ¢è¡Œï¼Œç«‹å³åˆ·æ–°è¾“å‡º

        # ç”Ÿæˆå›å¤ï¼ˆæµå¼ï¼‰
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id,
                streamer=streamer  # å…³é”®ï¼šæ·»åŠ æµå¼è¾“å‡ºå™¨
            )

        print()  # æµå¼è¾“å‡ºç»“æŸåæ¢è¡Œ


def chat_with_model_manual():
    """æ‰‹åŠ¨å®ç°æµå¼è¾“å‡ºçš„æ›¿ä»£æ–¹æ¡ˆï¼ˆæ›´çµæ´»ï¼‰"""
    print("\n" + "=" * 50)
    print("å¼€å§‹å¯¹è¯ - æ‰‹åŠ¨æµå¼è¾“å‡º (è¾“å…¥ 'quit' æˆ– 'é€€å‡º' ç»“æŸ)")
    print("=" * 50)

    while True:
        user_input = input("\nğŸ‘¤ ä½ : ").strip()
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            break

        # æ„å»ºæç¤ºè¯
        prompt = f"ç”¨æˆ·: {user_input}\nåŠ©æ‰‹:"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        print("ğŸ¤– AI: ", end="", flush=True)

        # æ‰‹åŠ¨å®ç°æµå¼ç”Ÿæˆ
        generated_tokens = []
        with torch.no_grad():
            # ä½¿ç”¨ generate ä½†é€æ­¥è·å–ç»“æœ
            for output in model.generate(
                    **inputs,
                    max_new_tokens=200,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    return_dict_in_generate=True,
                    output_scores=True
            ):
                # è·å–æœ€æ–°ç”Ÿæˆçš„token
                new_token = output.sequences[0, -1:]
                generated_tokens.append(new_token.item())

                # è§£ç å¹¶æ‰“å°
                new_text = tokenizer.decode(new_token, skip_special_tokens=True)
                print(new_text, end="", flush=True)

                # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                if new_token.item() == tokenizer.eos_token_id:
                    break

        print()  # ç»“æŸæ¢è¡Œ


# å…ˆåšä¸€ä¸ªå¿«é€Ÿæµ‹è¯•ï¼ˆæµå¼ï¼‰
print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•ï¼ˆæµå¼è¾“å‡ºï¼‰...")
test_prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹äººå·¥æ™ºèƒ½"
inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)

# åˆ›å»ºæµ‹è¯•ç”¨çš„æµå¼è¾“å‡ºå™¨
test_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

print("æµ‹è¯•å›å¤: ", end="", flush=True)
with torch.no_grad():
    test_outputs = model.generate(
        **inputs,
        max_new_tokens=100,
        do_sample=True,
        temperature=0.7,
        streamer=test_streamer
    )

print()  # æµ‹è¯•ç»“æŸæ¢è¡Œ

# é€‰æ‹©æµå¼è¾“å‡ºæ–¹å¼
print("\nè¯·é€‰æ‹©æµå¼è¾“å‡ºæ–¹å¼:")
print("1. è‡ªåŠ¨æµå¼è¾“å‡ºï¼ˆæ¨èï¼‰")
print("2. æ‰‹åŠ¨æµå¼è¾“å‡ºï¼ˆæ›´çµæ´»ï¼‰")
choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()

if choice == "2":
    chat_with_model_manual()
else:
    chat_with_model()

# æ¸…ç†GPUå†…å­˜
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print("\nğŸ§¹ å·²æ¸…ç†GPUå†…å­˜")